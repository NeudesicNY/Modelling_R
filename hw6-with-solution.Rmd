---
title: 'CSCI E-63C Week 6 Assignment: Solution'
output: html_document
---

```{r setup, include=FALSE}
library(cluster)
library(ISLR)
library(MASS)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
```

# Preface

In this assignment we will exercise some of the unsupervised learning approaches on [2016 Global Health Observatory data](http://www.who.int/gho/publications/world_health_statistics/2016/en/).  It is available at that website in the form of [Excel file](http://www.who.int/entity/gho/publications/world_health_statistics/2016/whs2016_AnnexB.xls?ua=1), but its cleaned up version ready for import into R for further analyses is available at CSCI E-63c canvas course web site [whs2016_AnnexB-data-wo-NAs.txt](https://canvas.harvard.edu/files/2939385/download?download_frd=1).  The cleaning and reformatting included: merging data from the two parts of Annex B, reducing column headers to one line with short tags, removal of ">", "<" and whitespaces, conversion to numeric format and replacement of undefined values (as indicated by en-dash'es in the Excel) with corresponding averages of those attributes.  The code that was used to format merged data is shown at the end of this document for your reference only.  You are advised to save yourself that trouble and start from preformatted text file available at the course website as shown above.  The explicit mapping of short variable names to their full description as provided in the original file is available in Excel file [whs2016_AnnexB-reformatted.xls](https://canvas.harvard.edu/files/2939383/download?download_frd=1) also available on the course canvas page.  Lastly, you are advised to download a local copy of this text file to your computer and access it there (as opposed to relying on R ability to establish URL connection to canvas that potentially requires login etc.)

Short example of code shown below illustrates reading this data from a local copy on your computer (assuming it has been copied into current working directory of your R session -- `getwd()` and `setwd()` commands are helpful to find out what is it currently and change it to desired location) and displaying summaries and pairs plot of five (out of almost 40) arbitrary chosen variables.  This is done for illustration purposes only -- the problems in the assignment expect use of all variables in this dataset.

```{r WHS}
whsAnnBdatNum <- read.table("whs2016_AnnexB-data-wo-NAs.txt",sep="\t",header=TRUE,quote="")
summary(whsAnnBdatNum[,c(1,4,7,10,17)])
pairs(whsAnnBdatNum[,c(1,4,7,10,17)])
```

In some way this dataset is somewhat similar to the `USArrests` dataset extensively used in ISLR labs and exercises -- it collects various continuous statistics characterizing human population across different territories.  It is several folds larger though -- instead of `r nrow(USArrests)` US states and `r ncol(USArrests)` attributes in `USArrests`, world health statistics (WHS) data characterizes `r nrow(whsAnnBdatNum)` WHO member states by `r ncol(whsAnnBdatNum)` variables.  Have fun!

The following problems are largely modeled after labs and exercises from Chapter 10 ISLR.  If anything presents a challenge, besides asking questions on piazza (that is always a good idea!), you are also encouraged to review corresponding lab sections in ISLR Chapter 10.

# Problem 1: Principal components analysis (PCA) (25 points)

## Sub-problem 1a: means and variances of WHS attributes (5 points)

Compare means and variances of the attributes in the world health statisics dataset -- plot of variance vs. mean is probably the best given number of attributes in the dataset.  Function `apply` allows to apply desired function (e.g. `mean` or `var`) to each row or column in the table.  Do you see all `r ncol(whsAnnBdatNum)` attributes in the plot, or at least most of them?  (Remember that you can use `plot(inpX,inpY,log="xy")` to use log-scale on both horizontal and vertical axes.)  What is the range of means and variances when calculated on untransformed data?  Which are the top two attributes with highest mean or variance?  What are the implications for PCA rendition of this dataset (in two dimensions) if applied to untransformed data?

### Solution

```{r,fig.width=12,fig.height=6}
range(apply(whsAnnBdatNum,2,mean))
range(apply(whsAnnBdatNum,2,var))
old.par <- par(mfrow=c(1,2),ps=16)
for ( axLog in c("","xy") ) {
  plot(apply(whsAnnBdatNum,2,mean),apply(whsAnnBdatNum,2,var),log=axLog,xlab="Average",ylab="Variance")
  text(apply(whsAnnBdatNum,2,mean),apply(whsAnnBdatNum,2,var),colnames(whsAnnBdatNum))
}
par(old.par)
```

Averages and variances of untransformed attributes in this dataset range from less than one to about $10^{9}$ and $10^{15}$ respectively.  Two attributes with the highest averages and variances are `r paste(names(sort(apply(whsAnnBdatNum,2,mean),decreasing=TRUE)[1:2]),collapse=" and ")`.  The PCA results for untransformed data can be expected to be by far predominantly driven by these two attributes with the largest variance.  To state more equitably about contributions of other attributes in this dataset they would have to be scaled and/or transformed.

## Sub-problem 1b: PCA on untransformed data (10 points)

Perform PCA on *untransformed* data in WHS dataset (remember, you can use R function `prcomp` for that).  Generate scree plot of PCA results (by calling `plot` on the result of `prcomp`) and plot of the two first principal components using `biplot`.  Which variables seem to predominantly drive the results of PCA when applied to untransformed data?

Please note that in this case you should expect `biplot` to generate substantial number of warnings.  Usually in R we should pay attention to these and understand whether they indicate that something went wrong in our analyses.  In this particular case they are expected -- why do you think that is?

The field `rotation` in the output of `prcomp` contains *loadings* of the 1st, 2nd, etc. principal components (PCs) -- that can interpreted as contributions of each of the attributes in the input data to each of the PCs.  What attributes have the largest (by their absolute value) loadings for the first and second principal component?  How does it compare to what you have observed when comparing means and variances of all attributes in the world health statistics dataset?

Calculate percentage of variance explained (PVE) by the first five principal components (PCs).  You can find an example of doing this in ISLR Chapter 10.4 (Lab 1 on PCA).

Lastly, perform PCA on *transposed* (but still *untransformed*) WHS dataset -- remember that in R `t(x)` returns transpose of `x`:

```{r}
matrix(1:6,ncol=3)
t(matrix(1:6,ncol=3))
```

Present results of PCA on transposed world health statistics dataset in the form of scree and biplot, describe the results.

### Solution

We know that "zero-length arrow is of indeterminate angle and so skipped" warning here is due to insufficient resolution of the plot to draw miniscule arrows in the figure dominated by the single attribute with orders of magnitude higher variance and turn them off to prevent cluttering of the report by setting `warning=FALSE` in the markdown triple-backticks clause:

```{r,warning=FALSE,fig.width=12,fig.height=6}
old.par <- par(mfrow=c(1,2),ps=16)
plot(prcomp(whsAnnBdatNum))
biplot(prcomp(whsAnnBdatNum))
par(old.par)
```

```{r}
sort(abs(prcomp(whsAnnBdatNum)$rotation[,1]),decreasing=T)[1:5]
sort(abs(prcomp(whsAnnBdatNum)$rotation[,2]),decreasing=T)[1:5]
100*prcomp(whsAnnBdatNum)$sdev[1:5]^2/sum(prcomp(whsAnnBdatNum)$sdev^2)
```

As we saw from the variance vs. mean plots, INTINTDS and TOTPOP explain by far the most of the variability in the untransformed data. As we can see from the numerical values of loadings the first two principal components more or less coincide with each of them respectively. The first principal component explain approximately four orders of magnitude more variance than the second one, and the second one explains about that much more than the third.

```{r,warning=FALSE,fig.width=12,fig.height=6}
old.par <- par(mfrow=c(1,2),ps=16)
plot(prcomp(t(whsAnnBdatNum)))
biplot(prcomp(t(whsAnnBdatNum)))
par(old.par)
sort(abs(prcomp(t(whsAnnBdatNum))$rotation[,1]),decreasing=T)[1:5]
sort(abs(prcomp(t(whsAnnBdatNum))$rotation[,2]),decreasing=T)[1:5]
```

Same conclusions from PCA on transposed *untransformed* data -- INTINTDS and TOTPOP are the variables that are the most different from all others and the first two principal components are greatly influenced by the two countries with most extreme values of these two attributes.


## Sub-problem 1c: PCA on scaled WHS data (10 points)

Perform PCA on scaled world health statistics data.  To do that you can either use as input to `prcomp` the output of`scale` as applied to the WHS data matrix or call `prcomp` with parameter `scale` set to `TRUE`.  Present results of PCA in the form of scree plot and plot of the first two principal components.  How do they compare to those generated on the results of PCA of *untransformed* data?  What dataset attributes contribute the most (by absolute value) to the top two PCs?  What are the signs of those contributions?  How would you interpret that?

The output of `biplot` with almost 200 text labels on it is pretty busy and could be tough to read.  You can achieve better control when plotting PCA results if instead you plot the first two columns of the `x` attribute in the output of `prcomp` -- e.g. `plot(prcomp(USArrests,scale=T)$x[,1:2])`.  Use this to label a subset of countries on the plot -- you can use `text` function in R to add labels at specified positions on the plot -- please feel free to choose several countries of your preference and discuss the results.  Alternatively, indicate US, UK, China, India, Mexico, Australia, Israel, Italy, Ireland and Sweden and discuss the results.  Where do the countries you have plotted fall in the graph?  Considering what you found out about contributions of different attributes to the first two PCs, what do their positions tell us about their (dis-)similarities in terms of associated health statistics?

Finally, perform PCA on *transposed* scaled WHS dataset -- present results in the form of scree plot and biplot and discuss these presentations.

### Solution

```{r WHSprcompPCA,fig.width=12,fig.height=6}
# scaled data:
old.par <- par(mfrow=c(1,2),ps=16)
plot(prcomp(scale(whsAnnBdatNum)))
biplot(prcomp(scale(whsAnnBdatNum)))
par(old.par)
```

```{r pcastatelbls,fig.width=12,fig.height=6}
plot(prcomp(scale(whsAnnBdatNum))$x[,1:2])
tmpSelStates <- c("UnitedStatesofAmerica","UnitedKingdom","Sweden","Portugal","Norway","NewZealand","Mexico","Japan","Italy","Israel","Ireland","India","Iceland","Germany","France","Finland","Denmark","China","Canada","Australia","Austria")
text(prcomp(scale(whsAnnBdatNum))$x[tmpSelStates,1:2],tmpSelStates)
```

```{r}
prcomp(scale(whsAnnBdatNum))$sdev[1:5]^2/sum(prcomp(scale(whsAnnBdatNum))$sdev^2)
sort(prcomp(scale(whsAnnBdatNum))$rotation[,1],decreasing=TRUE)[1:5]
sort(prcomp(scale(whsAnnBdatNum))$rotation[,1],decreasing=FALSE)[1:5]
```

The 1st PC explains about 40% of variance in the scaled data; mortality related measures are positively correlated and life expectancy related measures are negatively correlated with the first principal component.  Countries expected to score high on health statistics measures - e.g. Western Europe, US, Canada, Australia, Israel, Japan, New Zealand - form a cluster on the left, corresponding to more negative values of the 1st principal component, that in its turn anti-correlated with life expectancy related measurements.  

```{r pcatscale,fig.width=12,fig.height=6}
old.par <- par(mfrow=c(1,2),ps=16)
plot(prcomp(t(scale(whsAnnBdatNum))))
biplot(prcomp(t(scale(whsAnnBdatNum))))
par(old.par)
```

The first principal component also explain most of variance in transposed matrix.  larger biplot presented below for better resolution shows separate groups of variables related to life expectancy and mortality respectively. It also suggests that countries such as Angola, Chad and South Sudan (and others) score higher on mortality and Australia, Sweden and Switzerland (and others) score higher on life expectancy related measurements.

```{r pcatscalebiplot,fig.width=12,fig.height=12}
biplot(prcomp(t(scale(whsAnnBdatNum))))
```

### For *extra 8 points*

Try the following:

* Instead of scaling (or prior to scaling) perform log-transform of the data before passing it on to `prcomp`.  Given that some of the attributes have zero values, you will have to decide how to handle those to avoid negative infinity upon log-transformation.  Usually, a small positive (often a fraction of smallest non-zero value for that attribute) value is added to all (or just zero) values of the attribute that contains zeroes.  Present and describe the results.
* Demonstrate equivalence of the results as obtained by `prcomp(x)` and `cmdscale(dist(x))` where `x` represents scaled WHS dataset.
* Explore use of multidimensional scaling (MDS) tools available in library `MASS` such as `sammon` and `isoMDS`.  Present their results and discuss the differences between them and PCA output.  No, there was nothing on that in the lecture -- thus it is for extra points and to broaden your horizons.

### Solution

```{r extraptspcalog,fig.width=12,fig.height=6}
#  for extra points:
# PCA on log-transformed data (decide on the offset to avoid NaNs):
whsAnnBdatNumWo0s <- whsAnnBdatNum
for ( iCol in 1:ncol(whsAnnBdatNumWo0s) ) {
  # replace zeroes with half of the smallest non-zero:
  if ( sum(whsAnnBdatNumWo0s[,iCol]==0) > 0 ) {
    whsAnnBdatNumWo0s[whsAnnBdatNumWo0s[,iCol]==0,iCol] <- min(whsAnnBdatNumWo0s[whsAnnBdatNumWo0s[,iCol]>0,iCol])/2
  }
}
plot(apply(log(whsAnnBdatNumWo0s),2,mean),apply(log(whsAnnBdatNumWo0s),2,sd))
text(apply(log(whsAnnBdatNumWo0s),2,mean),apply(log(whsAnnBdatNumWo0s),2,sd),colnames(whsAnnBdatNumWo0s))
plot(prcomp(log(whsAnnBdatNumWo0s))$x[,1:2])
old.par <- par(mfrow=c(1,2),ps=16)
plot(prcomp(log(whsAnnBdatNumWo0s)))
biplot(prcomp(log(whsAnnBdatNumWo0s)))
par(old.par)
sort(prcomp(log(whsAnnBdatNumWo0s))$rotation[,1],decreasing=TRUE)[1:5]
sort(prcomp(log(whsAnnBdatNumWo0s))$rotation[,1],decreasing=FALSE)[1:5]
old.par <- par(mfrow=c(1,2),ps=16)
plot(prcomp(log(whsAnnBdatNumWo0s),scale=TRUE))
biplot(prcomp(log(whsAnnBdatNumWo0s),scale=TRUE))
par(old.par)
sort(prcomp(log(whsAnnBdatNumWo0s),scale=TRUE)$rotation[,1],decreasing=TRUE)[1:5]
sort(prcomp(log(whsAnnBdatNumWo0s),scale=TRUE)$rotation[,1],decreasing=FALSE)[1:5]
```

Use of log-transformed data decreases correlation between average and variance of the attributes and allows more comparable contributions of all attributes in the dataset to the estimated directions of principal components than that for untransformed data.  The loadings of the WHS attributes to the first principal component are different between PCA on log-transformed and PCA on scaled original data.  After scaling they become roughly comparable (with the opposite sign, but then the choice of sign of principal component is arbitrary) to those obtained on scaled original data.

```{r extraptspcamds}
# prcomp and cmdscale:
prcomp(scale(whsAnnBdatNum))$x[1:3,1:5]
cmdscale(dist(scale(whsAnnBdatNum)))[1:3,]
range(prcomp(scale(whsAnnBdatNum))$x[,1:2]-cmdscale(dist(scale(whsAnnBdatNum))))
```

PCA on a given input and classic MDS on euclidean distances between observations in the same dataset result in exactly the same (up to the sign) projection of the dataset onto lower-dimensional space.

```{r extrapts3mds,fig.width=15,fig.height=5}
# MASS library,  cmdscale, sammon, isoMDS, spearman
nms2lbl <- c("France","UnitedStatesofAmerica","Germany","Italy","India","UnitedKingdom","China","Congo","Chad","Mexico","Brazil","Namibia","Botswana")
old.par <- par(mfcol=c(2,3),ps=16)
dTmp <- dist(scale(whsAnnBdatNum))
for ( iTmp in 1:3 ) {
  x <- cmdscale(dTmp)
  if ( iTmp == 2 ) {
    x <- sammon(dTmp)$points
  }
  if ( iTmp == 3 ) {
    x <- isoMDS(dTmp)$points
  }
  plot(x,main=c("cmdscale","sammon","isoMDS")[iTmp])
  text(x[nms2lbl,],nms2lbl)
  plot(dTmp,dist(x),xlab="Original distance",ylab="Distance in 2D",main=c("cmdscale","sammon","isoMDS")[iTmp])
  mtext(paste("rho =",signif(cor(dTmp,dist(x),method="spearman"),3)))
  #text(x,rownames(x))
}
par(old.par)
```

Unlike PCA/classic MDS that project original dataset onto subspace defined by principal components, `sammon` and `isoMDS` implement different optimization approaches minimizing disagreement between distances among observations in lower dimensional space and those among them in the original data.  Similarity between their output and that from classic MDS is in part due to the fact that by default they use PCA/classic MDS results as starting approximation.  In this case, with default parameters, `isoMDS` appears to generate 2D representation that approximates distances in the space of original attributes most accurately.


# Problem 2: K-means clustering (15 points)

## Sub-problem 2a: k-means clusters of different size (5 points)

Using function `kmeans` perform K-means clustering on *explicitly scaled* (e.g. `kmeans(scale(x),2)`) world health statistics data for 2, 3 and 4 clusters.  Use `cluster` attribute in the output of `kmeans` to indicate cluster membership by color and/or shape of the corresponding symbols in the plot of the first two principal components generated independently on the same (scaled WHS) data.  E.g. `plot(prcomp(xyz)$x[,1:2],col=kmeans(xyz,4)$cluster)` where `xyz` is input data.  Describe the results.  Which countries are clustered together for each of these choices of $K$?

### Solution

```{r kmeans234,fig.width=15,fig.height=5}
old.par <- par(mfrow=c(1,3))
pcTmp <- prcomp(whsAnnBdatNum,scale=T)
for ( iTmp in 2:4 ) {
  kmTmp <- kmeans(scale(whsAnnBdatNum),iTmp)
  plot(pcTmp$x[,1:2],col=kmTmp$cluster,pch=kmTmp$cluster,main=iTmp)
  cat("k =",iTmp,fill=TRUE)
  print(lapply(unstack(data.frame(rownames(whsAnnBdatNum),kmTmp$cluster)),paste,collapse=","))
}
par(old.par)
```

Overall, the grouping by K-means appears to be closely related to the observation coordinates on the first principal components and therefore probably reflects variability among the countries in WHS dataset on life expectancy/mortality related measures.  Identities of the countries associated with these clusters likely correspond to the differences in thse aspects of health statistics.

## Sub-problem 2b: variability of k-means clustering (5 points)

By default, k-means clustering uses random set of centers as initial guesses of cluster centers.  Here we will explore variability of k-means cluster membership across several such initial random guesses.  To make such choices of random centers reproducible, we will use function `set.seed` to reset random number generator (RNG) used in R to make those initial guesses to known/controlled initial state.

Using the approach defined above, repeat k-means clustering with four clusters three times resetting RNG each time with `set.seed` using seeds of 1, 2 and 3 respectively.  Indicate cluster membership in each of these three trials on the plot of the first two principal components using color and/or shape as described above.  Two fields in the output of `kmeans` -- `tot.withinss` and `betweenss` -- characterize within and between clusters sum-of-squares.  Tighter clustering results are those which have smaller ratio of within to between sum-of-squares.  What are the resulting ratios of within to between sum-of-squares for each of these three k-means clustering results (with random seeds of 1, 2 and 3)?

Please bear in mind that the actual cluster identity is assigned randomly and does not matter -- i.e. if cluster 1 from the first run of `kmeans` (with random seed of 1) and cluster 4 from the run with the random seed of 2 contain the same observations (country/states in case of WHS dataset), they are *the same* clusters.


## Sub-problem 2c: effect of `nstarts` parameter (5 points)

Repeat the procedure implemented for the previous sub-problem (k-means with four clusters for RNG seeds of 1, 2 and 3) now using 100 as `nstart` parameter in the call to `kmeans`.  Represent results graphically as before.  How does cluster membership compare between those three runs now?  What is the ratio of within to between sum-of-squares in each of these three cases?  What is the impact of using higher than 1 (default) value of `nstart`?  What is the ISLR recommendation on this offered in Ch. 10.5.1?

### Solution

```{r WHSkmeans,fig.width=12,fig.height=8}
old.par <- par(mfcol=c(2,3))
pcTmp <- prcomp(whsAnnBdatNum,scale=T)
for ( iSeed in 1:3 ) {
  for ( tmpNstart in c(1,100) ) {
    set.seed(iSeed)
    kmTmp <- kmeans(scale(whsAnnBdatNum),4,nstart=tmpNstart)
    plot(pcTmp$x[,1:2],col=kmTmp$cluster,pch=kmTmp$cluster,main=paste(4,iSeed,signif(kmTmp$tot.withinss / kmTmp$betweenss,6)))
    mtext(paste("nstart =",tmpNstart))
  }
}
par(old.par)
```

Use of `nstart=1` results in three different groupings obtained in three repeated runs of the algorithm (as indicated by the colors and shapes of the points belonging to different clusters in the plots and by the values of ratios of the within to between cluster sums of squares).  Using `nstart=100` results in K-means returning the best clustering obtained in this many attempts (with the lowest ratio of within to between sum of squares) and yields the same grouping in each of the three repeats.  ISLR recommends using sufficiently high `nstart` for better stability of the result.

### For *extra 8 points*

Try the following:

* evaluate dependency between the stability of k-means clustering and the number of clusters and values of `nstarts`; to make this more quantitative consider using contingency table (i.e. `table`) to quantify concordance of two different clustering results (E.g. how many non-zero cells would be in the output of `table` for two perfectly concordant clustering assignments?)
* Try using `silhouette` from the library `cluster` as another tool for assessing cluster strength for some of the clusters obtained here and describe the results

### Solution

```{r kmeansknstart,fig.width=12,fig.height=6}
# various k and nstart:
dfTmp <- NULL
for ( tmpNstart in c(1,2,5,10,20,50) ) {
  for ( kTmp in 2:5 ) {
    lTmp <- list()
    for ( iSim in 1:10 ) {
      kmTmp <- kmeans(scale(whsAnnBdatNum),kTmp,nstart=tmpNstart)
      lTmp[[iSim]] <- kmTmp$cluster
    }
    for ( iTmp in 1:(length(lTmp)-1) ) {
      for ( jTmp in (iTmp+1):length(lTmp) ) {
        tblTmp <- table(lTmp[[iTmp]],lTmp[[jTmp]])
        dfTmp <- rbind(dfTmp,data.frame(k=kTmp,n=tmpNstart,nzCnt=sum(tblTmp!=0)))
      }
    }
  }
}
ggplot(dfTmp,aes(x=factor(n),y=nzCnt-k,fill=factor(k),colour=factor(k))) + geom_boxplot(fill="white",outlier.colour = NA) + geom_point(position=position_jitterdodge()) + xlab("N(starts)") + ylab("Count of inconsistencies")
```

The higher is the number of clusters the larger is the number of clustering trials that is required to obtain reproducible results.

```{r silhouette}
# silhouette:
set.seed(1)
kmTmp <- kmeans(scale(whsAnnBdatNum),2,nstart=100)
plot(silhouette(kmTmp$cluster,dist(scale(whsAnnBdatNum))))
kmTmp <- kmeans(scale(whsAnnBdatNum),4,nstart=100)
plot(silhouette(kmTmp$cluster,dist(scale(whsAnnBdatNum))))
kmTmp <- kmeans(scale(whsAnnBdatNum),8,nstart=100)
plot(silhouette(kmTmp$cluster,dist(scale(whsAnnBdatNum))))
```

Use of `k=2` seems to result in the most concordant grouping of the observations (with the highest average silhouette width).


# Problem 3: Hierarchical clustering (20 points)

## Sub-problem 3a: hierachical clustering by different linkages (10 points)

Cluster country states in (scaled) world health statistics data using default (Euclidean) distance and "complete", "average", "single" and "ward" linkages in the call to `hclust`.  Plot each clustering hierarchy, describe the differences.  For comparison, plot results of clustering *untransformed* WHS data using default parameters (Euclidean distance, "complete" linkage) -- discuss the impact of the scaling on the outcome of hierarchical clustering.

### Solution

```{r hc4,fig.width=12,fig.height=6}
xTmp <- scale(whsAnnBdatNum)
rownames(xTmp) <- substring(rownames(xTmp),1,15)
dTmp <- dist(xTmp)
plot(hclust(dTmp,method="complete"))
plot(hclust(dTmp,method="average"))
plot(hclust(dTmp,method="single"))
plot(hclust(dTmp,method="ward.D2"))
xTmp <- whsAnnBdatNum
rownames(xTmp) <- substring(rownames(xTmp),1,15)
dTmp <- dist(xTmp)
plot(hclust(dTmp))
```

Ward clustering appears to result in two top most clearly resolved clusters (as compared to those by complete, average and single linkages).  Results of hierarchical clustering of untransformed data is again predominantly affected by the attribute(s) with the highest variance.

## Sub-problem 3b: compare k-means and hierarchical clustering (5 points)

Using function `cutree` on the output of `hclust` determine assignment of the countries in WHS dataset into top four clusters when using Euclidean distance and "complete" linkage.  Use function `table` to compare membership of these clusters to those produced by k-means clustering with four clusters in the Problem 2(c) above.  Discuss the results.

### Solution

```{r}
kmTmp <- kmeans(scale(whsAnnBdatNum),4,nstart=100)
table(kmTmp$cluster,cutree(hclust(dist(scale(whsAnnBdatNum)),method="complete"),k=4))
```

Top four clusters by complete hierarchical clustering represent two pairs of kmeans clusters at `k=4` merged together and two other observations added to them one at a time.

## Sub-problem 3c: cluster variables by correlation (5 points)

Use (casted as distance) one-complement of Spearman correlation between *attributes* in world health statistics dataset to cluster *attributes* of WHS dataset.  E.g. `hclust(as.dist(1-cor(xyz,method="spearman")))` would cluster columns (as opposed to rows) in the matrix `xyz`.  Plot the results -- which variables tend to cluster together, why do you think that is?  Compare results obtained by this approach for scaled and untransformed WHS dataset?  How do they compare? What do you think is the explanation?

### Solution

```{r WHSprcompHC}
plot(hclust(as.dist(1-cor(whsAnnBdatNum,method="spearman")),method="ward.D2"))
plot(hclust(as.dist(1-cor(scale(whsAnnBdatNum),method="spearman")),method="ward.D2"))
```

Clustering of the WHS attributes by correlation between them results in two top level clusters, one enriched for life expectancy and another one - for mortality related attributes.  Use of Spearman rank correlation as measure of similarity for clustering attributes in scaled and untransformed WHS data yields exactly the same clustering trees, because calculation of correlation coefficient is invariant to scaling and centering of the attributes.

### For *extra 4 points*

Use contingency tables to compare cluster memberships for several top clusters across different choices of linkage (e.g. "complete","ward","single") and distance (Euclidean, Manhattan, one-complement of correlation coefficient).  Discuss the results.

### Solution

```{r}
lTmp <- list()
for ( iDist in c("euclidean","manhattan","spearman") ) {
  xTmp <- scale(whsAnnBdatNum)
  rownames(xTmp) <- substring(rownames(xTmp),1,15)
  if ( iDist == "spearman" ) {
    dTmp <- as.dist(1-cor(t(xTmp),method="spearman"))
  } else {
    dTmp <- dist(xTmp,method=iDist)
  }
  for ( iMthd in c("complete","average","single","ward.D2") ) {
    lTmp[[paste(iDist,iMthd)]] <- hclust(dTmp,method=iMthd)
  }
}
kTop <- 5
resTmp <- list()
for ( iTmp in 1:(length(lTmp)-1) ) {
  for ( jTmp in (iTmp+1):length(lTmp) ) {
    resTmp[[paste(names(lTmp)[iTmp],'-',names(lTmp)[jTmp])]] <- sum(table(cutree(lTmp[[iTmp]],k=kTop),cutree(lTmp[[jTmp]],k=kTop))!=0)-kTop
  }
}
sort(unlist(resTmp))[1:10]
sort(unlist(resTmp),decreasing=T)[1:10]
```

For `r kTop` top clusters most concordant groupings are indicated by the low numbers above (counts of non-zero cells in contingency table less number of clusters) and the least concordant -- by the high numbers.  

# Appendix: pre-processing of WHS data

```{r WHSpreproc}
whsAnnBdat <- read.table("../data/whs2016_AnnexB-data.txt",sep="\t",header=T,as.is=T,quote="")
dim(whsAnnBdat)
whsAnnBdat <- apply(whsAnnBdat,2,function(x)gsub(">","",gsub("<","",gsub(" ","",x))))
whsAnnBdat <- apply(whsAnnBdat,2,function(x){x[x==rawToChar(as.raw(150))]<-"";x})
rownames(whsAnnBdat) <- whsAnnBdat[,1]
whsAnnBdat <- whsAnnBdat[,-1]
whsAnnBdatNum <- apply(whsAnnBdat,2,as.numeric)
whsAnnBdatNum <- apply(whsAnnBdatNum,2,function(x){x[is.na(x)] <- mean(x,na.rm = TRUE);x})
rownames(whsAnnBdatNum) <- rownames(whsAnnBdat)
write.table(whsAnnBdatNum,"../data/whs2016_AnnexB-data-wo-NAs.txt",quote=F,sep="\t")
```

